{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/12136850/tab-delimited-file-using-csv-reader-not-delimiting-where-i-expect-it-to/12137264\n",
    "training_set = pd.read_csv(\"mediaeval-2015-trainingset.txt\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = pd.read_csv(\"mediaeval-2015-testset.txt\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-09-11 17:10:26+0000', tz='UTC')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-05-05 21:16:12+0000', tz='UTC')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set['timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18032"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape[0] + testing_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId                                          tweetText  \\\n",
       "0  263046056240115712  ¿Se acuerdan de la película: “El día después d...   \n",
       "1  262995061304852481  @milenagimon: Miren a Sandy en NY!  Tremenda i...   \n",
       "2  262979898002534400  Buena la foto del Huracán Sandy, me recuerda a...   \n",
       "3  262996108400271360     Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4  263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "\n",
       "  label lang  \n",
       "0  fake   es  \n",
       "1  fake   es  \n",
       "2  fake   es  \n",
       "3  fake   en  \n",
       "4  fake   en  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS.... htt...</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse fr...</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>“@Shyman33: Eclipse from ISS.... http://t.co/C...</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS.... http://t.co/En87OtvsU6</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: Éclipse vue de l'ISS... Autre chos...</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId                                          tweetText  \\\n",
       "0  578854927457349632  kereeen RT @Shyman33: Eclipse from ISS.... htt...   \n",
       "1  578874632670953472  Absolutely beautiful! RT @Shyman33: Eclipse fr...   \n",
       "2  578891261353984000  “@Shyman33: Eclipse from ISS.... http://t.co/C...   \n",
       "3  578846612312748032        Eclipse from ISS.... http://t.co/En87OtvsU6   \n",
       "4  578975333841551360  @ebonfigli: Éclipse vue de l'ISS... Autre chos...   \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "\n",
       "  label lang  \n",
       "0  fake   en  \n",
       "1  fake   en  \n",
       "2  fake   en  \n",
       "3  fake   en  \n",
       "4  fake   fr  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3755, 7)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14277, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2906"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set['tweetText'].map(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tweetId, tweetText, userId, imageId(s), username, timestamp, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[training_set.tweetText.map(lambda x: len(x)) > 280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(x):\n",
    "    if re.match(\"^\\w{3}\\s\\w{3}\\s\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\", x):\n",
    "        print(x)\n",
    "        return datetime.strptime(x, \"%a %b %d %H:%M:%S %z %Y\")    \n",
    "    else:\n",
    "        return datetime.strptime(x, \"%a %b %d %H: %M: %S %z %Y\")\n",
    "training_set['timestamp'] = training_set['timestamp'].apply(parse_date)\n",
    "testing_set['timestamp'] = testing_set['timestamp'].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'humor', 'real'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6742, 7)\n",
      "(2614, 7)\n",
      "(4921, 7)\n"
     ]
    }
   ],
   "source": [
    "print(training_set[training_set['label']=='fake'].shape)\n",
    "print(training_set[training_set['label']=='humor'].shape)\n",
    "print(training_set[training_set['label']=='real'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2546, 7)\n",
      "(1209, 7)\n"
     ]
    }
   ],
   "source": [
    "print(testing_set[testing_set['label']=='fake'].shape)\n",
    "print(testing_set[testing_set['label']=='real'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.813705132008305"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_set[training_set.label == \"fake\"]['username'].map(lambda x: len(x)).sum()/6742)\n",
    "print(training_set[training_set.label == \"humor\"]['username'].map(lambda x: len(x)).sum()/2614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.813705132\n",
      "10.6752104055\n",
      "10.5791505792\n"
     ]
    }
   ],
   "source": [
    "print(training_set[training_set.label == \"fake\"]['username'].map(lambda x: len(x)).sum()/6742)\n",
    "print(training_set[training_set.label == \"humor\"]['username'].map(lambda x: len(x)).sum()/2614)\n",
    "print(training_set[training_set.label == \"real\"]['username'].map(lambda x: len(x)).sum()/4921)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.332678711704634"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set[testing_set.label == \"fake\"]['username'].map(lambda x: len(x)).sum()/2546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.944582299421009"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set[testing_set.label == \"real\"]['username'].map(lambda x: len(x)).sum()/1209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'real'], dtype=object)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "def lang_test(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "        return lang\n",
    "    except LangDetectException:\n",
    "        print(\"Failed to detect: \" + x)\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect: Man sandy Foreal??  ⚡⚡⚡☔☔⚡🌊🌊☁🚣⛵💡🔌🚬🚬🚬🔫🔫🔒🔒🔐🔑🔒🚪🚪🚪🔨🔨🔨🏊🏊🏊🏊🎣🎣🎣😱😰😖😫😩😤💨💨💨💨💦💦💦💧💦💥💥💥👽💩🙌🙌🙌🙌🙌🏃🏃🏃🏃🏃👫👭💏👪👪👬👭💑🙇🌕🌕🌕🌎 http://t.co/vEWVXy10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'es': 1302,\n",
       "         'en': 11159,\n",
       "         'sq': 7,\n",
       "         'ru': 61,\n",
       "         'it': 98,\n",
       "         'no': 37,\n",
       "         'fr': 218,\n",
       "         'bg': 9,\n",
       "         'ro': 9,\n",
       "         'nl': 89,\n",
       "         'pt': 161,\n",
       "         'de': 129,\n",
       "         'tl': 318,\n",
       "         'pl': 35,\n",
       "         'id': 179,\n",
       "         'cy': 115,\n",
       "         'ja': 21,\n",
       "         'ar': 79,\n",
       "         'vi': 13,\n",
       "         'ca': 32,\n",
       "         'hu': 5,\n",
       "         'sv': 45,\n",
       "         'so': 122,\n",
       "         'fi': 14,\n",
       "         'sk': 11,\n",
       "         'da': 28,\n",
       "         'af': 69,\n",
       "         'el': 5,\n",
       "         'lt': 4,\n",
       "         'he': 1,\n",
       "         'hr': 5,\n",
       "         'ko': 7,\n",
       "         'tr': 34,\n",
       "         'zh-cn': 10,\n",
       "         'fa': 3,\n",
       "         'sl': 6,\n",
       "         'et': 8,\n",
       "         'sw': 11,\n",
       "         'lv': 2,\n",
       "         'th': 18,\n",
       "         'cs': 1,\n",
       "         'unknown': 1,\n",
       "         'mk': 1,\n",
       "         'hi': 1})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(training_set['tweetText'].apply(lang_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_testing_set = pd.Series(testing_set['tweetText'].apply(lang_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect: Man sandy Foreal??  ⚡⚡⚡☔☔⚡🌊🌊☁🚣⛵💡🔌🚬🚬🚬🔫🔫🔒🔒🔐🔑🔒🚪🚪🚪🔨🔨🔨🏊🏊🏊🏊🎣🎣🎣😱😰😖😫😩😤💨💨💨💨💦💦💦💧💦💥💥💥👽💩🙌🙌🙌🙌🙌🏃🏃🏃🏃🏃👫👭💏👪👪👬👭💑🙇🌕🌕🌕🌎 http://t.co/vEWVXy10\n"
     ]
    }
   ],
   "source": [
    "lang_training_set = pd.Series(training_set['tweetText'].apply(lang_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['new_label'] = training_set['label'].apply(lambda x: 'fake' if x == 'humor' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set['lang'] = lang_testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['lang'] = lang_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'es': 674,\n",
       "         'en': 5172,\n",
       "         'sq': 4,\n",
       "         'bg': 7,\n",
       "         'it': 32,\n",
       "         'da': 11,\n",
       "         'fr': 77,\n",
       "         'nl': 33,\n",
       "         'pt': 118,\n",
       "         'de': 36,\n",
       "         'tl': 125,\n",
       "         'cy': 56,\n",
       "         'id': 91,\n",
       "         'ru': 37,\n",
       "         'ja': 17,\n",
       "         'ar': 40,\n",
       "         'no': 13,\n",
       "         'af': 36,\n",
       "         'vi': 6,\n",
       "         'ca': 9,\n",
       "         'hu': 3,\n",
       "         'sv': 13,\n",
       "         'so': 41,\n",
       "         'fi': 8,\n",
       "         'pl': 18,\n",
       "         'hr': 4,\n",
       "         'sk': 9,\n",
       "         'el': 3,\n",
       "         'lt': 1,\n",
       "         'he': 1,\n",
       "         'ko': 5,\n",
       "         'tr': 9,\n",
       "         'zh-cn': 7,\n",
       "         'ro': 2,\n",
       "         'fa': 1,\n",
       "         'et': 2,\n",
       "         'sw': 1,\n",
       "         'th': 18,\n",
       "         'unknown': 1,\n",
       "         'hi': 1})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(training_set[training_set['label']=='fake']['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'es': 1296,\n",
       "         'en': 10962,\n",
       "         'sq': 7,\n",
       "         'bg': 9,\n",
       "         'it': 100,\n",
       "         'da': 27,\n",
       "         'fr': 216,\n",
       "         'nl': 89,\n",
       "         'pt': 165,\n",
       "         'de': 125,\n",
       "         'tl': 306,\n",
       "         'cy': 121,\n",
       "         'id': 182,\n",
       "         'ru': 60,\n",
       "         'ja': 22,\n",
       "         'ar': 79,\n",
       "         'no': 37,\n",
       "         'af': 67,\n",
       "         'vi': 13,\n",
       "         'ca': 33,\n",
       "         'hu': 5,\n",
       "         'sv': 39,\n",
       "         'so': 122,\n",
       "         'fi': 14,\n",
       "         'pl': 36,\n",
       "         'hr': 6,\n",
       "         'sk': 17,\n",
       "         'el': 5,\n",
       "         'lt': 5,\n",
       "         'he': 1,\n",
       "         'ko': 7,\n",
       "         'tr': 32,\n",
       "         'zh-cn': 10,\n",
       "         'ro': 6,\n",
       "         'fa': 3,\n",
       "         'sl': 9,\n",
       "         'et': 9,\n",
       "         'sw': 12,\n",
       "         'th': 19,\n",
       "         'unknown': 1,\n",
       "         'mk': 1,\n",
       "         'hi': 1,\n",
       "         'lv': 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(training_set['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'en': 4078,\n",
       "         'pl': 13,\n",
       "         'it': 37,\n",
       "         'tl': 93,\n",
       "         'af': 19,\n",
       "         'es': 277,\n",
       "         'fr': 94,\n",
       "         'nl': 14,\n",
       "         'no': 14,\n",
       "         'id': 15,\n",
       "         'tr': 9,\n",
       "         'th': 1,\n",
       "         'so': 52,\n",
       "         'cy': 30,\n",
       "         'pt': 13,\n",
       "         'ja': 3,\n",
       "         'fi': 2,\n",
       "         'bg': 1,\n",
       "         'et': 3,\n",
       "         'vi': 2,\n",
       "         'sv': 20,\n",
       "         'lt': 2,\n",
       "         'de': 60,\n",
       "         'ca': 10,\n",
       "         'ru': 14,\n",
       "         'sk': 5,\n",
       "         'da': 10,\n",
       "         'ar': 21,\n",
       "         'mk': 1,\n",
       "         'ro': 2,\n",
       "         'sq': 2,\n",
       "         'fa': 1,\n",
       "         'sl': 1,\n",
       "         'el': 2})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(training_set[training_set['label']=='real']['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'en': 1712,\n",
       "         'fr': 45,\n",
       "         'es': 345,\n",
       "         'it': 31,\n",
       "         'ca': 14,\n",
       "         'de': 29,\n",
       "         'nl': 42,\n",
       "         'so': 29,\n",
       "         'cy': 35,\n",
       "         'tl': 88,\n",
       "         'ar': 18,\n",
       "         'id': 76,\n",
       "         'sl': 8,\n",
       "         'af': 12,\n",
       "         'tr': 14,\n",
       "         'da': 6,\n",
       "         'no': 10,\n",
       "         'fi': 4,\n",
       "         'pt': 34,\n",
       "         'hr': 2,\n",
       "         'vi': 5,\n",
       "         'pl': 5,\n",
       "         'ru': 9,\n",
       "         'sw': 11,\n",
       "         'sv': 6,\n",
       "         'et': 4,\n",
       "         'ja': 2,\n",
       "         'bg': 1,\n",
       "         'sk': 3,\n",
       "         'ro': 2,\n",
       "         'hu': 2,\n",
       "         'lt': 2,\n",
       "         'fa': 1,\n",
       "         'zh-cn': 3,\n",
       "         'sq': 1,\n",
       "         'ko': 2,\n",
       "         'lv': 1})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(training_set[training_set['label']=='humor']['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'en': 1617,\n",
       "         'fr': 22,\n",
       "         'es': 39,\n",
       "         'ca': 6,\n",
       "         'it': 14,\n",
       "         'fi': 4,\n",
       "         'id': 3,\n",
       "         'nl': 21,\n",
       "         'sq': 3,\n",
       "         'ar': 178,\n",
       "         'af': 4,\n",
       "         'hr': 1,\n",
       "         'tr': 12,\n",
       "         'hi': 3,\n",
       "         'ta': 1,\n",
       "         'el': 2,\n",
       "         'sv': 2,\n",
       "         'so': 521,\n",
       "         'pt': 36,\n",
       "         'vi': 2,\n",
       "         'ko': 1,\n",
       "         'ja': 3,\n",
       "         'ro': 1,\n",
       "         'bg': 1,\n",
       "         'tl': 2,\n",
       "         'cy': 5,\n",
       "         'lt': 1,\n",
       "         'pl': 1,\n",
       "         'de': 40})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(testing_set[testing_set['label']=='fake']['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[(training_set['lang']=='es') & (training_set['label']=='real')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'en': 1144,\n",
       "         'fr': 10,\n",
       "         'pt': 3,\n",
       "         'it': 5,\n",
       "         'es': 24,\n",
       "         'nl': 3,\n",
       "         'tl': 1,\n",
       "         'ar': 4,\n",
       "         'pl': 1,\n",
       "         'sq': 3,\n",
       "         'hi': 2,\n",
       "         'te': 2,\n",
       "         'ru': 1,\n",
       "         'th': 2,\n",
       "         'id': 3,\n",
       "         'sv': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(testing_set[testing_set['label']=='real']['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetId       False\n",
       "tweetText     False\n",
       "userId        False\n",
       "imageId(s)    False\n",
       "username      False\n",
       "timestamp     False\n",
       "label         False\n",
       "lang          False\n",
       "new_label     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['tweetText'].apply(lambda x: print(\"%s\\n%s\\n\"%(x, detect(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK parse tweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿Se acuerdan de la película: “El día después de mañana”?',\n",
       " 'Me recuerda a lo que está pasando con el huracán #Sandy.',\n",
       " 'http://t.co/JQQeRPwN']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(training_set.tweetText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿Se', 'acuerdan', 'de', 'la', 'película', ':', '“', 'El', 'día', 'después', 'de', 'mañana', '”', '?']\n",
      "['Me', 'recuerda', 'a', 'lo', 'que', 'está', 'pasando', 'con', 'el', 'huracán', '#', 'Sandy', '.']\n",
      "['http', ':', '//t.co/JQQeRPwN']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(training_set.tweetText[0]):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_en = training_set[training_set.lang == \"en\"]\n",
    "testing_set_en = testing_set[testing_set.lang == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scary shit #hurricane #NY http://t.co/e4JLBUfH\n",
      "['Scary', 'shit', '#', 'hurricane', '#', 'NY', 'http', ':', '//t.co/e4JLBUfH']\n"
     ]
    }
   ],
   "source": [
    "print(training_set_en['tweetText'][3])\n",
    "print(word_tokenize(training_set_en['tweetText'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "len(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetText1 = list(map(str.lower, word_tokenize(training_set.tweetText[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿se', 'acuerdan', 'de', 'la', 'película', '“', 'el', 'día', 'después', 'de', 'mañana', '”', 'recuerda', 'lo', 'que', 'está', 'pasando', 'con', 'el', 'huracán', 'sandy', 'http', '//t.co/jqqerpwn']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in tweetText1 if word not in stopwords_en_withpunct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With combined stopwords:\n",
      "['Scary', 'shit', 'hurricane', 'NY', 'http', '//t.co/e4JLBUfH']\n"
     ]
    }
   ],
   "source": [
    "print('With combined stopwords:')\n",
    "print([word for word in word_tokenize(training_set_en['tweetText'][3]) if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Scary', 'NNP'),\n",
       " ('shit', 'VBD'),\n",
       " ('#', '#'),\n",
       " ('hurricane', 'NN'),\n",
       " ('#', '#'),\n",
       " ('NY', 'NNP'),\n",
       " ('http', 'NN'),\n",
       " (':', ':'),\n",
       " ('//t.co/e4JLBUfH', 'NN')]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(training_set_en['tweetText'][3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿se',\n",
       " 'acuerdan',\n",
       " 'de',\n",
       " 'la',\n",
       " 'película',\n",
       " ':',\n",
       " '“',\n",
       " 'el',\n",
       " 'día',\n",
       " 'después',\n",
       " 'de',\n",
       " 'mañana',\n",
       " '”',\n",
       " '?',\n",
       " 'me',\n",
       " 'recuerda',\n",
       " 'a',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'está',\n",
       " 'pasando',\n",
       " 'con',\n",
       " 'el',\n",
       " 'huracán',\n",
       " '#',\n",
       " 'sandy',\n",
       " '.',\n",
       " 'http',\n",
       " ':',\n",
       " '//t.co/jqqerpwn']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_sent(training_set.tweetText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sandy',\n",
       " 'newyork',\n",
       " 'hurricane',\n",
       " 'statueofliberty',\n",
       " 'usa',\n",
       " 'http',\n",
       " '//t.co/iqfebo1e']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(training_set_en.tweetText[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['milenagimon', 'miren', 'sandy', 'en', 'ny', 'tremenda', 'imagen', 'del', 'huracán', 'parece', 'el', '``', 'día', 'de', 'la', 'independencia', \"''\", 'http', '//t.co/41juweux', 'real', 'rt']\n",
      "Word counts:\n",
      "Counter({'milenagimon': 1, 'miren': 1, 'sandy': 1, 'en': 1, 'ny': 1, 'tremenda': 1, 'imagen': 1, 'del': 1, 'huracán': 1, 'parece': 1, 'el': 1, '``': 1, 'día': 1, 'de': 1, 'la': 1, 'independencia': 1, \"''\": 1, 'http': 1, '//t.co/41juweux': 1, 'real': 1, 'rt': 1})\n"
     ]
    }
   ],
   "source": [
    "print(tweet2)\n",
    "print('Word counts:')\n",
    "print(Counter(tweet2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with StringIO('\\n'.join([training_set.tweetText[0], training_set.tweetText[1]])) as fin:\n",
    "    # Create the vectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit_transform(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'se': 31,\n",
       " 'acuerdan': 1,\n",
       " 'de': 4,\n",
       " 'la': 16,\n",
       " 'película': 25,\n",
       " 'el': 8,\n",
       " 'día': 7,\n",
       " 'después': 6,\n",
       " 'mañana': 18,\n",
       " 'me': 19,\n",
       " 'recuerda': 28,\n",
       " 'lo': 17,\n",
       " 'que': 26,\n",
       " 'está': 10,\n",
       " 'pasando': 24,\n",
       " 'con': 3,\n",
       " 'huracán': 12,\n",
       " 'sandy': 30,\n",
       " 'http': 11,\n",
       " 'co': 2,\n",
       " 'jqqerpwn': 15,\n",
       " 'milenagimon': 20,\n",
       " 'miren': 21,\n",
       " 'en': 9,\n",
       " 'ny': 22,\n",
       " 'tremenda': 32,\n",
       " 'imagen': 13,\n",
       " 'del': 5,\n",
       " 'parece': 23,\n",
       " 'independencia': 14,\n",
       " '41juweux': 0,\n",
       " 'real': 27,\n",
       " 'rt': 29}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with StringIO('\\n'.join([training_set.tweetText[0], training_set.tweetText[1]])) as fin:\n",
    "    # Override the analyzer totally with our preprocess text\n",
    "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
    "                                 tokenizer=word_tokenize)\n",
    "    count_vect.fit_transform(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'¿se': 32,\n",
       " 'acuerdan': 5,\n",
       " 'de': 7,\n",
       " 'la': 18,\n",
       " 'película': 26,\n",
       " '“': 33,\n",
       " 'el': 11,\n",
       " 'día': 10,\n",
       " 'después': 9,\n",
       " 'mañana': 20,\n",
       " '”': 34,\n",
       " 'recuerda': 28,\n",
       " 'lo': 19,\n",
       " 'está': 13,\n",
       " 'pasando': 25,\n",
       " 'con': 6,\n",
       " 'huracán': 15,\n",
       " 'sandy': 30,\n",
       " 'http': 14,\n",
       " '//t.co/jqqerpwn': 2,\n",
       " 'milenagimon': 21,\n",
       " 'miren': 22,\n",
       " 'en': 12,\n",
       " 'ny': 23,\n",
       " 'tremenda': 31,\n",
       " 'imagen': 16,\n",
       " 'del': 8,\n",
       " 'parece': 24,\n",
       " '``': 4,\n",
       " 'independencia': 17,\n",
       " '2': 3,\n",
       " \"''\": 0,\n",
       " '//t.co/41juweux': 1,\n",
       " 'real': 27,\n",
       " 'rt': 29}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix/Vectors:\n",
      " [[0 0 1 0 0 1 1 2 0 1 1 2 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1]\n",
      " [1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Matrix/Vectors:\\n', count_vect.transform([training_set.tweetText[0], training_set.tweetText[1]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "train_set = count_vect.fit_transform(training_set[training_set.lang==\"en\"].tweetText)\n",
    "train_tags = training_set[training_set.lang==\"en\"].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(analyzer=preprocess_text', \n",
    "                                  stop_words=stoplist_combined,                                      \n",
    "                                  ngram_range=(2,2),\n",
    "                                  max_features=300,\n",
    "                                  encoding='utf-8',\n",
    "                                  decode_error='strict',\n",
    "                                  strip_accents = None,\n",
    "                                  lowercase=True,\n",
    "                                  tokenizer=word_tokenize\n",
    "                                  )\n",
    "\n",
    "# word_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "#     analyzer='word',\n",
    "#     token_pattern=r'\\w{1,}',\n",
    "#     stop_words=stoplist_combined,\n",
    "#     ngram_range=(2, 2),\n",
    "#     max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    \n",
    "    def build_analyzer(self):\n",
    "        \n",
    "        stop_words = self.get_stop_words()\n",
    "        \n",
    "        def analyser(doc):\n",
    "            \n",
    "            if (self.lowercase == True):\n",
    "                doc = doc.lower()\n",
    "            tokens = lemmatize_sent(doc)\n",
    "            \n",
    "            return(self._word_ngrams(tokens, stop_words))\n",
    "        return (analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist_combined.add('”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = CustomVectorizer(stop_words=stoplist_combined,                                      \n",
    "                                  ngram_range=(1,3),\n",
    "                                  max_features=1000,\n",
    "                                  encoding='utf-8',\n",
    "                                  decode_error='strict',\n",
    "                                  strip_accents = None,\n",
    "                                  lowercase=True,\n",
    "                                  tokenizer=word_tokenize\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'”' in stoplist_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_features = word_vectorizer.fit_transform(training_set_en['tweetText'])\n",
    "testing_word_features = word_ vectorizer.transform(testing_set_en['tweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              term         rank\n",
      "341                           http  1130.982746\n",
      "698                          sandy  1066.661233\n",
      "722                     sandy http   712.486753\n",
      "353                      hurricane   585.552595\n",
      "360                hurricane sandy   467.572438\n",
      "371                 hurricanesandy   455.966937\n",
      "540                            nyc   310.616491\n",
      "683                             rt   296.914606\n",
      "18                             ...   287.003794\n",
      "373            hurricanesandy http   265.050867\n",
      "8                               's   248.130663\n",
      "778                          shark   210.827840\n",
      "579                          photo   210.159165\n",
      "364           hurricane sandy http   207.353219\n",
      "980                           york   184.634617\n",
      "799                          sochi   164.133960\n",
      "806                        soldier   149.717827\n",
      "517                        newyork   148.378357\n",
      "603                        picture   140.924589\n",
      "536                             ny   132.577507\n",
      "501                            n't   131.072783\n",
      "902                           tomb   126.530020\n",
      "595                            pic   125.750856\n",
      "655                           real   125.560971\n",
      "928                        unknown   123.558225\n",
      "847                          storm   122.842204\n",
      "963                            wow   122.148268\n",
      "185                          crazy   117.542037\n",
      "400                         jersey   116.459821\n",
      "425                        liberty   116.117280\n",
      "..                             ...          ...\n",
      "941             vacation hurricane     6.223491\n",
      "942     vacation hurricane katrina     6.223491\n",
      "506                 n't yet- check     6.158486\n",
      "976                           yet-     6.158486\n",
      "977                     yet- check     6.158486\n",
      "978                yet- check week     6.158486\n",
      "505                       n't yet-     6.158486\n",
      "276             front yard flooded     5.981777\n",
      "642              pretty incredible     5.971156\n",
      "666        remember president bush     5.896762\n",
      "665             remember president     5.896762\n",
      "139        bush vacation hurricane     5.896762\n",
      "138                  bush vacation     5.896762\n",
      "137                           bush     5.896762\n",
      "638        president bush vacation     5.896762\n",
      "637                 president bush     5.896762\n",
      "413              katrina president     5.896762\n",
      "414        katrina president obama     5.896762\n",
      "357    hurricane katrina president     5.896762\n",
      "643          pretty incredible n't     5.778292\n",
      "392            incredible n't yet-     5.778292\n",
      "391                 incredible n't     5.778292\n",
      "502                       n't live     5.564755\n",
      "434                     live place     5.564755\n",
      "503                 n't live place     5.564755\n",
      "693               rt stephgosk mob     5.515095\n",
      "210             dianesawyer pretty     4.896450\n",
      "561                  obama p2 http     4.733323\n",
      "640             president obama p2     4.733323\n",
      "211  dianesawyer pretty incredible     4.534587\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "terms = word_vectorizer.get_feature_names()\n",
    "sums = train_word_features.sum(axis=0)\n",
    "\n",
    "data = []\n",
    "for col, term in enumerate(terms):\n",
    "    data.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "print(ranking.sort_values('rank', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_vectorizer.vocabulary_.items(), key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 767)\t0.507643236917\n",
      "  (0, 791)\t0.44296794477\n",
      "  (0, 351)\t0.223965783717\n",
      "  (0, 535)\t0.415077176939\n",
      "  (0, 339)\t0.0948576430503\n",
      "  (0, 536)\t0.560918921398\n"
     ]
    }
   ],
   "source": [
    "print(train_word_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3062\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3063\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 100",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-8af492e46d3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_word_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2683\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2685\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2690\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2692\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2486\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3063\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3065\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 100"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(train_word_features.toarray(), columns=word_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = training_set[training_set.lang==\"en\"].new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set = count_vect.transform(testing_set[testing_set.lang==\"en\"].tweetText)\n",
    "test_tags = testing_set[testing_set.lang==\"en\"].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "kpca = KernelPCA(kernel='rbf', gamma=100, random_state=42)\n",
    "train_word_features_kpca = kpca.fit_transform(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsvd = TruncatedSVD()\n",
    "train_word_features_tsvd = tsvd.fit_transform(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(random_state=42)\n",
    "train_word_features_nmf = nmf.fit_transform(train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14541106,  0.05954198],\n",
       "       [ 0.19110331,  0.02895184],\n",
       "       [ 0.11547867,  0.04313842],\n",
       "       ..., \n",
       "       [ 0.0634501 ,  0.02603405],\n",
       "       [ 0.06970431,  0.02532622],\n",
       "       [ 0.07268181,  0.05379976]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_features_tsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11140, 1000)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_features_nmf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11140, 11037)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_features_kpca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11140, 1000)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.09923570e-02,   1.13195360e-02,  -9.72937626e-03, ...,\n",
       "         -5.48610652e-13,   8.97245913e-14,  -1.49775255e-12],\n",
       "       [ -2.09923570e-02,   1.13195360e-02,  -9.72937626e-03, ...,\n",
       "         -5.48610652e-13,   8.97245914e-14,  -1.49775255e-12],\n",
       "       [ -2.09923570e-02,   1.13195360e-02,  -9.72937626e-03, ...,\n",
       "         -5.48610650e-13,   8.97245915e-14,  -1.49775255e-12],\n",
       "       ..., \n",
       "       [ -2.09923570e-02,   1.13195360e-02,  -9.72937626e-03, ...,\n",
       "         -5.48610652e-13,   8.97245913e-14,  -1.49775255e-12],\n",
       "       [ -2.09923570e-02,   1.13195360e-02,  -9.72937626e-03, ...,\n",
       "         -5.48610652e-13,   8.97245913e-14,  -1.49775255e-12],\n",
       "       [ -2.09923570e-02,   1.13195360e-02,  -9.72937626e-03, ...,\n",
       "         -5.48610652e-13,   8.97245913e-14,  -1.49775255e-12]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_features_kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3755, 27942)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14277, 27942)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_word_features_nmf, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_set = clf.predict(testing_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67148014440433212"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=predicted_set, y_true=test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_pred=predicted_set, y_true=test_tags, pos_label=\"fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['real', 'fake'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predicted_set).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'real'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[training_set.lang==\"en\"].new_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'real'], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-4a356fcaf8a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mpredicted_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2678\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2679\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2680\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;31m# check_bool_indexer will throw exception if Series key cannot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2718\u001b[0m             \u001b[1;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2719\u001b[1;33m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2720\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2721\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(ax, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m             raise IndexingError('Unalignable boolean Series provided as '\n\u001b[0m\u001b[0;32m   2357\u001b[0m                                 \u001b[1;34m'indexer (index of the boolean Series and of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m                                 'the indexed object do not match')\n",
      "\u001b[1;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match"
     ]
    }
   ],
   "source": [
    "testing_set[testing_set[testing_set.lang=='en'].label != predicted_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>579016447747325953</td>\n",
       "      <td>@PicPedant ? @ObservingSpace Solar eclipse wit...</td>\n",
       "      <td>21036658</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>mike_rawlins</td>\n",
       "      <td>Fri Mar 20 20:27:33 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>578983474125746176</td>\n",
       "      <td>Holy cow! RT @ObservingSpace: Solar eclipse wi...</td>\n",
       "      <td>14090323</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>hauck</td>\n",
       "      <td>Fri Mar 20 18:16:31 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>578982912596426752</td>\n",
       "      <td>OMG RT @ObservingSpace: Solar eclipse with an ...</td>\n",
       "      <td>14807898</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>elakdawalla</td>\n",
       "      <td>Fri Mar 20 18:14:18 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>579029763483942912</td>\n",
       "      <td>WAOUH! \"@ObservingSpace: Solar eclipse with an...</td>\n",
       "      <td>17812024</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>Chergaoui</td>\n",
       "      <td>Fri Mar 20 21:20:28 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>579348690789359616</td>\n",
       "      <td>So who wins the Photobombing The Eclipse conte...</td>\n",
       "      <td>12794772</td>\n",
       "      <td>eclipse_10</td>\n",
       "      <td>KarlOnSea</td>\n",
       "      <td>Sat Mar 21 18:27:46 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>579172761463062528</td>\n",
       "      <td>Double partial eclipse: the ISS and the Moon\\n...</td>\n",
       "      <td>314515213</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>MilkyWay_Earth</td>\n",
       "      <td>Sat Mar 21 06:48:41 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>579280743198695425</td>\n",
       "      <td>凄い！月に縫い目が…\\nISSタイミング良すぎ！\\nRT @MilkyWay_Earth: ...</td>\n",
       "      <td>404074286</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>chayamontweets</td>\n",
       "      <td>Sat Mar 21 13:57:46 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>578848174128893952</td>\n",
       "      <td>Where are you watching the #eclipse from? If i...</td>\n",
       "      <td>1135540254</td>\n",
       "      <td>eclipse_07</td>\n",
       "      <td>businessratesav</td>\n",
       "      <td>Fri Mar 20 09:18:53 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>578996207684620288</td>\n",
       "      <td>There! Today's eclipse shadow from the ISS: ht...</td>\n",
       "      <td>395263528</td>\n",
       "      <td>eclipse_07</td>\n",
       "      <td>BrockworthBajr</td>\n",
       "      <td>Fri Mar 20 19:07:07 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>578996329982070784</td>\n",
       "      <td>There! Today's eclipse shadow from the ISS. ht...</td>\n",
       "      <td>14451705</td>\n",
       "      <td>eclipse_07</td>\n",
       "      <td>andygates</td>\n",
       "      <td>Fri Mar 20 19:07:36 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>579250609943089152</td>\n",
       "      <td>http://t.co/zkyjinLpH6 The solar eclipse of Ma...</td>\n",
       "      <td>1370114947</td>\n",
       "      <td>eclipse_07</td>\n",
       "      <td>recenttechnos</td>\n",
       "      <td>Sat Mar 21 11:58:02 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>578969456287830016</td>\n",
       "      <td>Solar eclipse with transit of the ISS - March ...</td>\n",
       "      <td>2431845505</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>CodeAndCommand</td>\n",
       "      <td>Fri Mar 20 17:20:49 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>578970914861113344</td>\n",
       "      <td>ISS solar transit during today's eclipse http:...</td>\n",
       "      <td>175834344</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>cineboxandrew</td>\n",
       "      <td>Fri Mar 20 17:26:37 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>578971322174304256</td>\n",
       "      <td>Solar eclipse with transit of the ISS - March ...</td>\n",
       "      <td>2907090723</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>subredditsbot</td>\n",
       "      <td>Fri Mar 20 17:28:14 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>578996157541531648</td>\n",
       "      <td>Solar eclipse with ISS transit. Very cool. htt...</td>\n",
       "      <td>3098326194</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>ekstansbery</td>\n",
       "      <td>Fri Mar 20 19:06:55 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>579014231992881152</td>\n",
       "      <td>Solar eclipse with an ISS transit over Spain C...</td>\n",
       "      <td>2866961017</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>tweet2014001</td>\n",
       "      <td>Fri Mar 20 20:18:45 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>579235655537983488</td>\n",
       "      <td>Partial solar eclipse with a transit of the IS...</td>\n",
       "      <td>2831444330</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>zumba8z</td>\n",
       "      <td>Sat Mar 21 10:58:36 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>579426495132786688</td>\n",
       "      <td>Solar eclipse and transit of ISS . . . Photogr...</td>\n",
       "      <td>2988550545</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>SpacePornBot</td>\n",
       "      <td>Sat Mar 21 23:36:56 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>579428022840901632</td>\n",
       "      <td>Solar eclipse and transit of ISS . . . Photogr...</td>\n",
       "      <td>3091792955</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>DeepestPictures</td>\n",
       "      <td>Sat Mar 21 23:43:00 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>579429799267037184</td>\n",
       "      <td>Solar eclipse and transit of ISS . . . Photogr...</td>\n",
       "      <td>2431845505</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>CodeAndCommand</td>\n",
       "      <td>Sat Mar 21 23:50:04 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>579431094837182465</td>\n",
       "      <td>Solar eclipse and transit of ISS . . . Photogr...</td>\n",
       "      <td>2905821363</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>redditpicsbot</td>\n",
       "      <td>Sat Mar 21 23:55:12 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>579431306586619905</td>\n",
       "      <td>Solar eclipse and transit of ISS . . . Photogr...</td>\n",
       "      <td>2907090723</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>subredditsbot</td>\n",
       "      <td>Sat Mar 21 23:56:03 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>579432863889641474</td>\n",
       "      <td>Solar eclipse and transit of ISS . . . Photogr...</td>\n",
       "      <td>2597329015</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>TheSpacePix</td>\n",
       "      <td>Sun Mar 22 00:02:14 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>579568183847174144</td>\n",
       "      <td>Impressive photo of the ISS passing in front o...</td>\n",
       "      <td>361781213</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>DannyNimmo</td>\n",
       "      <td>Sun Mar 22 08:59:57 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>579666015312867328</td>\n",
       "      <td>The ISS crosses in front of the sun during the...</td>\n",
       "      <td>79009461</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>jessicaplautz</td>\n",
       "      <td>Sun Mar 22 15:28:42 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>580188534977523713</td>\n",
       "      <td>@Cmdr_Hadfield just saw this; lunar eclipse wi...</td>\n",
       "      <td>48895642</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>jlessard9127</td>\n",
       "      <td>Tue Mar 24 02:05:00 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>580347309432541184</td>\n",
       "      <td>Partial Solar Eclipse and transit of ISS taken...</td>\n",
       "      <td>901473385</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>SoBadAstronomer</td>\n",
       "      <td>Tue Mar 24 12:35:55 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>583862106778423296</td>\n",
       "      <td>Partial solar eclipse with a transit of the IS...</td>\n",
       "      <td>127124449</td>\n",
       "      <td>eclipse_10</td>\n",
       "      <td>TVChannels4U</td>\n",
       "      <td>Fri Apr 03 05:22:28 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>578983155056644097</td>\n",
       "      <td>“Video of the ISS crossing the Sun DURING the ...</td>\n",
       "      <td>474068520</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>derosajoe51</td>\n",
       "      <td>Fri Mar 20 18:15:15 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>578972523884318720</td>\n",
       "      <td>ISS in transit during the eclipse today http:/...</td>\n",
       "      <td>2361361367</td>\n",
       "      <td>eclipse_08</td>\n",
       "      <td>SalamoneNino</td>\n",
       "      <td>Fri Mar 20 17:33:01 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>592627472128024576</td>\n",
       "      <td>Woman born in Saffron #Walden among the missin...</td>\n",
       "      <td>351873959</td>\n",
       "      <td>nepal_24</td>\n",
       "      <td>MsGilbert1805</td>\n",
       "      <td>Mon Apr 27 09:52:54 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>593448810975731713</td>\n",
       "      <td>Have an extra tent laying around? #Nepal homel...</td>\n",
       "      <td>2387671628</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>emily_CNN</td>\n",
       "      <td>Wed Apr 29 16:16:36 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>591945125829423104</td>\n",
       "      <td>#Nepal struck by massive earthquake. http://t....</td>\n",
       "      <td>22973101</td>\n",
       "      <td>nepal_28</td>\n",
       "      <td>goFBW</td>\n",
       "      <td>Sat Apr 25 12:41:30 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>593156388920000512</td>\n",
       "      <td>Pray for nepal #NepalEarthquake #NepalQuakeRel...</td>\n",
       "      <td>223936375</td>\n",
       "      <td>nepal_30</td>\n",
       "      <td>77vaghela</td>\n",
       "      <td>Tue Apr 28 20:54:38 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>592506435075969024</td>\n",
       "      <td>Nepal Hit By Another Earthquake http://t.co/I1...</td>\n",
       "      <td>87883018</td>\n",
       "      <td>nepal_27</td>\n",
       "      <td>Qieqieeisyummy</td>\n",
       "      <td>Mon Apr 27 01:51:57 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>591889297185214464</td>\n",
       "      <td>#NepalEarthQuake pics http://t.co/79s8nSMBoo</td>\n",
       "      <td>564524925</td>\n",
       "      <td>nepal_27</td>\n",
       "      <td>UttrakhandAjay</td>\n",
       "      <td>Sat Apr 25 08:59:39 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>591939073113067520</td>\n",
       "      <td>#earthquake #BlackDay #NepalQuake #NepalEarthq...</td>\n",
       "      <td>3069356726</td>\n",
       "      <td>nepal_27</td>\n",
       "      <td>kchetrip</td>\n",
       "      <td>Sat Apr 25 12:17:27 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>591891600260734976</td>\n",
       "      <td>Stay safe #NepalEarthquake http://t.co/z0vutz87lJ</td>\n",
       "      <td>542608982</td>\n",
       "      <td>nepal_27</td>\n",
       "      <td>Im_Rishad</td>\n",
       "      <td>Sat Apr 25 09:08:48 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>591872455339216896</td>\n",
       "      <td>My brother took this Pic, who is from Nepal. A...</td>\n",
       "      <td>212979601</td>\n",
       "      <td>nepal_27</td>\n",
       "      <td>Boskjos</td>\n",
       "      <td>Sat Apr 25 07:52:44 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>591931319585636352</td>\n",
       "      <td>More pics from the #NepalEarthquake http://t.c...</td>\n",
       "      <td>14813790</td>\n",
       "      <td>nepal_27</td>\n",
       "      <td>_sunilrawat</td>\n",
       "      <td>Sat Apr 25 11:46:38 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>591867788437753857</td>\n",
       "      <td>USGS has upgraded the magnitude of the #earthq...</td>\n",
       "      <td>20751449</td>\n",
       "      <td>nepal_22,nepal_25</td>\n",
       "      <td>the_hindu</td>\n",
       "      <td>Sat Apr 25 07:34:11 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>591867788563587073</td>\n",
       "      <td>USGS has upgraded the magnitude of the #earthq...</td>\n",
       "      <td>177829660</td>\n",
       "      <td>nepal_22,nepal_25</td>\n",
       "      <td>TheHindu</td>\n",
       "      <td>Sat Apr 25 07:34:11 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>591989147679621121</td>\n",
       "      <td>( ♥ #CesarAcosta ♥ ) Nepal's historic Dharahar...</td>\n",
       "      <td>3015956190</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>Ramon08e</td>\n",
       "      <td>Sat Apr 25 15:36:26 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>591989149202186240</td>\n",
       "      <td>( ♥ #CesarAcosta ♥ ) Nepal's historic Dharahar...</td>\n",
       "      <td>1599983358</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>Pedro_Mateo1</td>\n",
       "      <td>Sat Apr 25 15:36:26 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>592267545521979392</td>\n",
       "      <td>Nepal earthquake destroys historic 9-storey Dh...</td>\n",
       "      <td>279009908</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>MyAnandaBazar</td>\n",
       "      <td>Sun Apr 26 10:02:41 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>583803185829142528</td>\n",
       "      <td>Smfh RT @joansalihi: The horror image from ins...</td>\n",
       "      <td>170163629</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>803codered</td>\n",
       "      <td>Fri Apr 03 01:28:20 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>583598033880928256</td>\n",
       "      <td>Are You Christian Or Muslim? – Muslims Singlin...</td>\n",
       "      <td>19355955</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>Blazingcatfur</td>\n",
       "      <td>Thu Apr 02 11:53:08 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>583885490119692288</td>\n",
       "      <td>This is what's happened to our fellow Kenyan.....</td>\n",
       "      <td>1313734856</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>emmanyo4</td>\n",
       "      <td>Fri Apr 03 06:55:23 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>583855255487717377</td>\n",
       "      <td>WHY “@joansalihi: The horror image from inside...</td>\n",
       "      <td>160712322</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>_SLENDERella_</td>\n",
       "      <td>Fri Apr 03 04:55:15 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>583957869361373184</td>\n",
       "      <td>Every new attack is considered a wake up call....</td>\n",
       "      <td>473121474</td>\n",
       "      <td>garissa_03</td>\n",
       "      <td>BenazirKarim</td>\n",
       "      <td>Fri Apr 03 11:43:00 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>584027135251243008</td>\n",
       "      <td>If your \"god\" approves that you fight for him ...</td>\n",
       "      <td>570920371</td>\n",
       "      <td>garissa_03</td>\n",
       "      <td>FAmosworld</td>\n",
       "      <td>Fri Apr 03 16:18:14 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>584063967930232832</td>\n",
       "      <td>#Blacksarehumans #GarissaAttack Let's stand wi...</td>\n",
       "      <td>1598684982</td>\n",
       "      <td>garissa_03</td>\n",
       "      <td>eseblogger</td>\n",
       "      <td>Fri Apr 03 18:44:35 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>583784147358973952</td>\n",
       "      <td>RT @libertylynx: RT @alimhaider: was a time wh...</td>\n",
       "      <td>18793488</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>itsEric</td>\n",
       "      <td>Fri Apr 03 00:12:41 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>583834137187848192</td>\n",
       "      <td>Death toll reaches 147 in a kenyan university ...</td>\n",
       "      <td>796322448</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>Momin1998</td>\n",
       "      <td>Fri Apr 03 03:31:20 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>583875484661972992</td>\n",
       "      <td>#GarissaAttack May their soles rest in peace h...</td>\n",
       "      <td>270216745</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>Kennetholayo</td>\n",
       "      <td>Fri Apr 03 06:15:38 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>583883309488406528</td>\n",
       "      <td>Innocent souls just going to study..........#s...</td>\n",
       "      <td>355597630</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>CollinsBoadi</td>\n",
       "      <td>Fri Apr 03 06:46:43 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>583908772382429184</td>\n",
       "      <td>The horror that happened at #GarissaAttack. ht...</td>\n",
       "      <td>57579807</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>Vusani_</td>\n",
       "      <td>Fri Apr 03 08:27:54 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>583944163097468928</td>\n",
       "      <td>147 students killed in #GarissaAttack by #AlSh...</td>\n",
       "      <td>1652971309</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>frederikswennen</td>\n",
       "      <td>Fri Apr 03 10:48:32 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>583855980418625536</td>\n",
       "      <td>WHY “@joansalihi: The horror image from inside...</td>\n",
       "      <td>135668220</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>OhiOsRAW</td>\n",
       "      <td>Fri Apr 03 04:58:07 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>583783882182430722</td>\n",
       "      <td>In honor of our fellow Students, we pray the g...</td>\n",
       "      <td>2492068440</td>\n",
       "      <td>garissa_04</td>\n",
       "      <td>Instaadict</td>\n",
       "      <td>Fri Apr 03 00:11:38 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweetId                                          tweetText  \\\n",
       "149   579016447747325953  @PicPedant ? @ObservingSpace Solar eclipse wit...   \n",
       "152   578983474125746176  Holy cow! RT @ObservingSpace: Solar eclipse wi...   \n",
       "153   578982912596426752  OMG RT @ObservingSpace: Solar eclipse with an ...   \n",
       "154   579029763483942912  WAOUH! \"@ObservingSpace: Solar eclipse with an...   \n",
       "171   579348690789359616  So who wins the Photobombing The Eclipse conte...   \n",
       "177   579172761463062528  Double partial eclipse: the ISS and the Moon\\n...   \n",
       "178   579280743198695425  凄い！月に縫い目が…\\nISSタイミング良すぎ！\\nRT @MilkyWay_Earth: ...   \n",
       "184   578848174128893952  Where are you watching the #eclipse from? If i...   \n",
       "185   578996207684620288  There! Today's eclipse shadow from the ISS: ht...   \n",
       "186   578996329982070784  There! Today's eclipse shadow from the ISS. ht...   \n",
       "187   579250609943089152  http://t.co/zkyjinLpH6 The solar eclipse of Ma...   \n",
       "191   578969456287830016  Solar eclipse with transit of the ISS - March ...   \n",
       "192   578970914861113344  ISS solar transit during today's eclipse http:...   \n",
       "193   578971322174304256  Solar eclipse with transit of the ISS - March ...   \n",
       "195   578996157541531648  Solar eclipse with ISS transit. Very cool. htt...   \n",
       "199   579014231992881152  Solar eclipse with an ISS transit over Spain C...   \n",
       "204   579235655537983488  Partial solar eclipse with a transit of the IS...   \n",
       "208   579426495132786688  Solar eclipse and transit of ISS . . . Photogr...   \n",
       "209   579428022840901632  Solar eclipse and transit of ISS . . . Photogr...   \n",
       "210   579429799267037184  Solar eclipse and transit of ISS . . . Photogr...   \n",
       "211   579431094837182465  Solar eclipse and transit of ISS . . . Photogr...   \n",
       "212   579431306586619905  Solar eclipse and transit of ISS . . . Photogr...   \n",
       "213   579432863889641474  Solar eclipse and transit of ISS . . . Photogr...   \n",
       "216   579568183847174144  Impressive photo of the ISS passing in front o...   \n",
       "219   579666015312867328  The ISS crosses in front of the sun during the...   \n",
       "228   580188534977523713  @Cmdr_Hadfield just saw this; lunar eclipse wi...   \n",
       "229   580347309432541184  Partial Solar Eclipse and transit of ISS taken...   \n",
       "230   583862106778423296  Partial solar eclipse with a transit of the IS...   \n",
       "231   578983155056644097  “Video of the ISS crossing the Sun DURING the ...   \n",
       "235   578972523884318720  ISS in transit during the eclipse today http:/...   \n",
       "...                  ...                                                ...   \n",
       "1066  592627472128024576  Woman born in Saffron #Walden among the missin...   \n",
       "1087  593448810975731713  Have an extra tent laying around? #Nepal homel...   \n",
       "1094  591945125829423104  #Nepal struck by massive earthquake. http://t....   \n",
       "1101  593156388920000512  Pray for nepal #NepalEarthquake #NepalQuakeRel...   \n",
       "1115  592506435075969024  Nepal Hit By Another Earthquake http://t.co/I1...   \n",
       "1124  591889297185214464       #NepalEarthQuake pics http://t.co/79s8nSMBoo   \n",
       "1125  591939073113067520  #earthquake #BlackDay #NepalQuake #NepalEarthq...   \n",
       "1134  591891600260734976  Stay safe #NepalEarthquake http://t.co/z0vutz87lJ   \n",
       "1137  591872455339216896  My brother took this Pic, who is from Nepal. A...   \n",
       "1140  591931319585636352  More pics from the #NepalEarthquake http://t.c...   \n",
       "1142  591867788437753857  USGS has upgraded the magnitude of the #earthq...   \n",
       "1143  591867788563587073  USGS has upgraded the magnitude of the #earthq...   \n",
       "1416  591989147679621121  ( ♥ #CesarAcosta ♥ ) Nepal's historic Dharahar...   \n",
       "1418  591989149202186240  ( ♥ #CesarAcosta ♥ ) Nepal's historic Dharahar...   \n",
       "1590  592267545521979392  Nepal earthquake destroys historic 9-storey Dh...   \n",
       "1853  583803185829142528  Smfh RT @joansalihi: The horror image from ins...   \n",
       "1862  583598033880928256  Are You Christian Or Muslim? – Muslims Singlin...   \n",
       "1864  583885490119692288  This is what's happened to our fellow Kenyan.....   \n",
       "1870  583855255487717377  WHY “@joansalihi: The horror image from inside...   \n",
       "1883  583957869361373184  Every new attack is considered a wake up call....   \n",
       "1884  584027135251243008  If your \"god\" approves that you fight for him ...   \n",
       "1885  584063967930232832  #Blacksarehumans #GarissaAttack Let's stand wi...   \n",
       "1886  583784147358973952  RT @libertylynx: RT @alimhaider: was a time wh...   \n",
       "1888  583834137187848192  Death toll reaches 147 in a kenyan university ...   \n",
       "1891  583875484661972992  #GarissaAttack May their soles rest in peace h...   \n",
       "1898  583883309488406528  Innocent souls just going to study..........#s...   \n",
       "1900  583908772382429184  The horror that happened at #GarissaAttack. ht...   \n",
       "1901  583944163097468928  147 students killed in #GarissaAttack by #AlSh...   \n",
       "1902  583855980418625536  WHY “@joansalihi: The horror image from inside...   \n",
       "1920  583783882182430722  In honor of our fellow Students, we pray the g...   \n",
       "\n",
       "          userId          imageId(s)         username  \\\n",
       "149     21036658         eclipse_08      mike_rawlins   \n",
       "152     14090323         eclipse_08             hauck   \n",
       "153     14807898         eclipse_08       elakdawalla   \n",
       "154     17812024         eclipse_08         Chergaoui   \n",
       "171     12794772         eclipse_10         KarlOnSea   \n",
       "177    314515213          eclipse_08   MilkyWay_Earth   \n",
       "178    404074286         eclipse_08    chayamontweets   \n",
       "184   1135540254         eclipse_07   businessratesav   \n",
       "185    395263528         eclipse_07    BrockworthBajr   \n",
       "186     14451705         eclipse_07         andygates   \n",
       "187   1370114947         eclipse_07     recenttechnos   \n",
       "191   2431845505          eclipse_08   CodeAndCommand   \n",
       "192    175834344         eclipse_08     cineboxandrew   \n",
       "193   2907090723          eclipse_08    subredditsbot   \n",
       "195   3098326194         eclipse_08       ekstansbery   \n",
       "199   2866961017          eclipse_08     tweet2014001   \n",
       "204   2831444330          eclipse_08          zumba8z   \n",
       "208   2988550545         eclipse_08      SpacePornBot   \n",
       "209   3091792955         eclipse_08   DeepestPictures   \n",
       "210   2431845505         eclipse_08    CodeAndCommand   \n",
       "211   2905821363         eclipse_08     redditpicsbot   \n",
       "212   2907090723         eclipse_08     subredditsbot   \n",
       "213   2597329015         eclipse_08       TheSpacePix   \n",
       "216    361781213         eclipse_08        DannyNimmo   \n",
       "219     79009461         eclipse_08     jessicaplautz   \n",
       "228     48895642         eclipse_08      jlessard9127   \n",
       "229    901473385         eclipse_08   SoBadAstronomer   \n",
       "230    127124449         eclipse_10      TVChannels4U   \n",
       "231    474068520         eclipse_08       derosajoe51   \n",
       "235   2361361367         eclipse_08      SalamoneNino   \n",
       "...          ...                 ...              ...   \n",
       "1066   351873959           nepal_24     MsGilbert1805   \n",
       "1087  2387671628            nepal_25        emily_CNN   \n",
       "1094    22973101           nepal_28             goFBW   \n",
       "1101   223936375           nepal_30         77vaghela   \n",
       "1115    87883018           nepal_27    Qieqieeisyummy   \n",
       "1124   564524925           nepal_27    UttrakhandAjay   \n",
       "1125  3069356726           nepal_27          kchetrip   \n",
       "1134   542608982           nepal_27         Im_Rishad   \n",
       "1137   212979601            nepal_27          Boskjos   \n",
       "1140    14813790           nepal_27       _sunilrawat   \n",
       "1142    20751449  nepal_22,nepal_25         the_hindu   \n",
       "1143   177829660  nepal_22,nepal_25          TheHindu   \n",
       "1416  3015956190            nepal_25         Ramon08e   \n",
       "1418  1599983358            nepal_25     Pedro_Mateo1   \n",
       "1590   279009908           nepal_25     MyAnandaBazar   \n",
       "1853   170163629         garissa_04        803codered   \n",
       "1862    19355955         garissa_04     Blazingcatfur   \n",
       "1864  1313734856          garissa_04         emmanyo4   \n",
       "1870   160712322          garissa_04    _SLENDERella_   \n",
       "1883   473121474         garissa_03      BenazirKarim   \n",
       "1884   570920371         garissa_03        FAmosworld   \n",
       "1885  1598684982         garissa_03        eseblogger   \n",
       "1886    18793488          garissa_04          itsEric   \n",
       "1888   796322448          garissa_04        Momin1998   \n",
       "1891   270216745          garissa_04     Kennetholayo   \n",
       "1898   355597630          garissa_04     CollinsBoadi   \n",
       "1900    57579807          garissa_04          Vusani_   \n",
       "1901  1652971309          garissa_04  frederikswennen   \n",
       "1902   135668220          garissa_04         OhiOsRAW   \n",
       "1920  2492068440          garissa_04       Instaadict   \n",
       "\n",
       "                           timestamp label lang  \n",
       "149   Fri Mar 20 20:27:33 +0000 2015  real   en  \n",
       "152   Fri Mar 20 18:16:31 +0000 2015  real   en  \n",
       "153   Fri Mar 20 18:14:18 +0000 2015  real   en  \n",
       "154   Fri Mar 20 21:20:28 +0000 2015  real   en  \n",
       "171   Sat Mar 21 18:27:46 +0000 2015  real   en  \n",
       "177   Sat Mar 21 06:48:41 +0000 2015  real   en  \n",
       "178   Sat Mar 21 13:57:46 +0000 2015  real   en  \n",
       "184   Fri Mar 20 09:18:53 +0000 2015  real   en  \n",
       "185   Fri Mar 20 19:07:07 +0000 2015  real   en  \n",
       "186   Fri Mar 20 19:07:36 +0000 2015  real   en  \n",
       "187   Sat Mar 21 11:58:02 +0000 2015  real   en  \n",
       "191   Fri Mar 20 17:20:49 +0000 2015  real   en  \n",
       "192   Fri Mar 20 17:26:37 +0000 2015  real   en  \n",
       "193   Fri Mar 20 17:28:14 +0000 2015  real   en  \n",
       "195   Fri Mar 20 19:06:55 +0000 2015  real   en  \n",
       "199   Fri Mar 20 20:18:45 +0000 2015  real   en  \n",
       "204   Sat Mar 21 10:58:36 +0000 2015  real   en  \n",
       "208   Sat Mar 21 23:36:56 +0000 2015  real   en  \n",
       "209   Sat Mar 21 23:43:00 +0000 2015  real   en  \n",
       "210   Sat Mar 21 23:50:04 +0000 2015  real   en  \n",
       "211   Sat Mar 21 23:55:12 +0000 2015  real   en  \n",
       "212   Sat Mar 21 23:56:03 +0000 2015  real   en  \n",
       "213   Sun Mar 22 00:02:14 +0000 2015  real   en  \n",
       "216   Sun Mar 22 08:59:57 +0000 2015  real   en  \n",
       "219   Sun Mar 22 15:28:42 +0000 2015  real   en  \n",
       "228   Tue Mar 24 02:05:00 +0000 2015  real   en  \n",
       "229   Tue Mar 24 12:35:55 +0000 2015  real   en  \n",
       "230   Fri Apr 03 05:22:28 +0000 2015  real   en  \n",
       "231   Fri Mar 20 18:15:15 +0000 2015  real   en  \n",
       "235   Fri Mar 20 17:33:01 +0000 2015  real   en  \n",
       "...                              ...   ...  ...  \n",
       "1066  Mon Apr 27 09:52:54 +0000 2015  real   en  \n",
       "1087  Wed Apr 29 16:16:36 +0000 2015  real   en  \n",
       "1094  Sat Apr 25 12:41:30 +0000 2015  real   en  \n",
       "1101  Tue Apr 28 20:54:38 +0000 2015  real   en  \n",
       "1115  Mon Apr 27 01:51:57 +0000 2015  real   en  \n",
       "1124  Sat Apr 25 08:59:39 +0000 2015  real   en  \n",
       "1125  Sat Apr 25 12:17:27 +0000 2015  real   en  \n",
       "1134  Sat Apr 25 09:08:48 +0000 2015  real   en  \n",
       "1137  Sat Apr 25 07:52:44 +0000 2015  real   en  \n",
       "1140  Sat Apr 25 11:46:38 +0000 2015  real   en  \n",
       "1142  Sat Apr 25 07:34:11 +0000 2015  real   en  \n",
       "1143  Sat Apr 25 07:34:11 +0000 2015  real   en  \n",
       "1416  Sat Apr 25 15:36:26 +0000 2015  real   en  \n",
       "1418  Sat Apr 25 15:36:26 +0000 2015  real   en  \n",
       "1590  Sun Apr 26 10:02:41 +0000 2015  real   en  \n",
       "1853  Fri Apr 03 01:28:20 +0000 2015  real   en  \n",
       "1862  Thu Apr 02 11:53:08 +0000 2015  real   en  \n",
       "1864  Fri Apr 03 06:55:23 +0000 2015  real   en  \n",
       "1870  Fri Apr 03 04:55:15 +0000 2015  real   en  \n",
       "1883  Fri Apr 03 11:43:00 +0000 2015  real   en  \n",
       "1884  Fri Apr 03 16:18:14 +0000 2015  real   en  \n",
       "1885  Fri Apr 03 18:44:35 +0000 2015  real   en  \n",
       "1886  Fri Apr 03 00:12:41 +0000 2015  real   en  \n",
       "1888  Fri Apr 03 03:31:20 +0000 2015  real   en  \n",
       "1891  Fri Apr 03 06:15:38 +0000 2015  real   en  \n",
       "1898  Fri Apr 03 06:46:43 +0000 2015  real   en  \n",
       "1900  Fri Apr 03 08:27:54 +0000 2015  real   en  \n",
       "1901  Fri Apr 03 10:48:32 +0000 2015  real   en  \n",
       "1902  Fri Apr 03 04:58:07 +0000 2015  real   en  \n",
       "1920  Fri Apr 03 00:11:38 +0000 2015  real   en  \n",
       "\n",
       "[86 rows x 8 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set[testing_set.lang=='en'][(testing_set[testing_set.lang=='en'].label != predicted_set) & (testing_set[testing_set.lang=='en'].label == 'real')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
